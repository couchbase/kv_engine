{
    "params": {
        "allow_sanitize_value_in_deletion" : {
            "default" : "true",
            "descr": "Let EPE delete/prepare/del_with_meta prune any invalid body in the payload instead of failing",
            "dynamic" : true,
            "type" : "bool"
        },
        "alog_block_size": {
            "default": "4096",
            "descr": "Logging block size.",
            "dynamic": false,
            "type": "size_t",
            "requires": {
                "bucket_type": "persistent"
            }
        },
        "alog_path": {
            "default": "",
            "descr": "Path to the access log.",
            "dynamic": true,
            "type": "std::string",
            "requires": {
                "bucket_type": "persistent"
            }
        },
        "access_scanner_enabled": {
            "default": {
                "on-prem": "true",
                "serverless": "false"
            },
            "descr": "True if access scanner task is enabled",
            "dynamic": true,
            "type": "bool",
            "requires": {
                "bucket_type": "persistent"
            }
        },
        "alog_sleep_time": {
            "default": "1440",
            "descr": "Number of minutes between each sweep for the access log",
            "dynamic": true,
            "type": "size_t",
            "validator": {
                "range": {
                    "max": 4320,
                    "min": 1
                }
            },
            "requires": {
                "bucket_type": "persistent"
            }
        },
        "alog_task_time": {
            "default": "2",
            "descr": "Hour in GMT time when access scanner task is scheduled to run",
            "dynamic": true,
            "type": "size_t",
            "validator": {
                "range": {
                    "max": 23,
                    "min": 0
                }
            },
            "requires": {
                "bucket_type": "persistent"
            }
        },
        "alog_resident_ratio_threshold": {
            "default": "95",
            "desr": "Resident ratio percentage above which we do not generate access log",
            "dynamic": true,
            "type": "size_t",
            "validator": {
                "range": {
                    "max": 100,
                    "min": 0
                }
            },
            "requires": {
                "bucket_type": "persistent"
            }
        },
        "alog_max_stored_items": {
            "default": "1024",
            "desr": "The maximum number of items the Access Scanner will hold in memory before commiting them to disk",
            "type": "size_t",
            "dynamic": true,
            "validator": {
                "range": {
                    "min": 1
                }
            },
            "requires": {
                "bucket_type": "persistent"
            }
        },
        "backend": {
            "default": "couchdb",
            "desr": "The storage backend to use. The \"nexus\" variant is a test only backend which will run two backends for a single bucket and compare the results of their operations.",
            "dynamic": false,
            "type": "std::string",
            "validator": {
                "enum": [
                    "couchdb",
                    "magma",
                    "nexus"
                ]
            }
        },
        "backfill_mem_threshold": {
            "default": "96",
            "desr": "Memory usage threshold (percentage of bucket quota) after which backfill will be snoozed.",
            "dynamic": true,
            "type": "size_t",
            "validator": {
                "range": {
                    "max": 100,
                    "min": 0
                }
            }
        },
        "bfilter_enabled": {
            "default": "true",
            "desr": "Enable or disable the bloom filter",
            "dynamic": true,
            "type": "bool"
        },
        "bfilter_key_count": {
            "default": "10000",
            "desr": "Bloomfilter: Estimated key count per vbucket",
            "dynamic": true,
            "type": "size_t",
            "validator": {
                "range": {
                    "min": 1
                }
            }
        },
        "bfilter_fp_prob": {
            "default": "0.01",
            "desr": "Bloomfilter: Allowed probability for false positives",
            "dynamic": true,
            "type": "float"
        },
        "bfilter_residency_threshold": {
            "default": "0.1",
            "desr" : "If resident ratio (during full eviction) were found less than this threshold, compaction will include all items into bloomfilter",
            "dynamic": true,
            "type" : "float",
            "validator": {
                "range": {
                    "max": 1.0,
                    "min": 0.0
                }
            }
        },
        "bucket_type": {
            "default": "persistent",
            "descr": "Bucket type in the couchbase server",
            "dynamic": false,
            "type": "std::string",
            "validator": {
                "enum": [
                         "ephemeral",
                         "persistent"
                        ]
            }
        },
        "bucket_quota_change_task_poll_interval": {
            "default": "30",
            "descr": "Time in seconds between the BucketQuotaChangeTask polling memory usage to attempt to reduce the bucket quota",
            "dynamic": true,
            "type": "size_t"
        },
        "compaction_expire_from_start": {
            "default": "true",
            "descr": "Should compaction expire items that were logically deleted at the start of the compaction (true) or at the point in time at which they were visited (false)?",
            "dynamic": true,
            "type": "bool"
        },
        "chk_expel_enabled": {
            "default" : "true",
            "descr": "Enable the ability to expel (remove from memory) items from a checkpoint.  An item can be expelled if all cursors in the checkpoint have iterated past the item.",
            "dynamic" : true,
            "type": "bool"
        },
        "chk_remover_stime": {
            "default": "5",
            "dynamic": false,
            "type": "size_t"
        },
        "checkpoint_destruction_tasks": {
            "default": "2",
            "descr": "Number of tasks responsible for destroying closed unreferenced checkpoints.",
            "dynamic": true,
            "type": "size_t",
            "validator": {
                "range": {
                    "min": 1
                }
            }
        },
        "checkpoint_memory_ratio": {
            "default": "0.2",
            "descr": "Max ratio of the bucket quota that can be allocated in checkpoints. The system enters a TempOOM phase if hit.",
            "dynamic": true,
            "type": "float",
            "validator": {
                "range": {
                    "min": 0.0,
                    "max": 1.0
                }
            }
        },
        "checkpoint_memory_recovery_upper_mark": {
            "default": "0.9",
            "descr": "Fraction of the checkpoint quota (as computed by checkpoint_memory_ratio) that triggers attempt of memory releasing from checkpoint.",
            "dynamic": true,
            "type": "float",
            "validator": {
                "range": {
                    "min": 0.0,
                    "max": 1.0
                }
            }
        },
        "checkpoint_memory_recovery_lower_mark": {
            "default": "0.6",
            "descr": "Fraction of the checkpoint quota (as computed by checkpoint_memory_ratio) that represents the target of checkpoint memory recovery. Memory recovery yields when reached.",
            "dynamic": true,
            "type": "float",
            "validator": {
                "range": {
                    "min": 0.0,
                    "max": 1.0
                }
            }
        },
        "checkpoint_remover_task_count": {
            "default": "1",
            "descr": "Number of concurrent tasks performing ItemExpel and CursorDrop/CheckpointRemoval",
            "dynamic": false,
            "type": "size_t",
            "validator": {
                "range": {
                    "min": 1
                }
            }
        },
        "checkpoint_max_size": {
            "default": "0",
            "descr": "Max size (in bytes) of a single checkpoint. '0' for EPEngine auto-setup.",
            "dynamic": true,
            "type": "size_t"
        },
        "collections_drop_compaction_delay" : {
            "default": "5000",
            "descr": "How many milliseconds before compaction runs following the drop of a collection",
            "dynamic": false,
            "type": "size_t"
        },
        "collections_enabled" : {
            "default": "true",
            "descr": "Enable the collections functionality, enabling the storage of collection metadata",
            "dynamic": false,
            "type": "bool"
        },
        "compaction_expiry_fetch_inline" : {
            "default": "true",
            "descr": "If compaction requires a bgfetch before attempting expiry to ensure it does not expire an older version of the document, true: fetch it in the compaction thread. false: queue a bgfetch for the bgfetcher task to complete",
            "dynamic": true,
            "type": "bool"
        },
        "compression_mode": {
            "default": "off",
            "descr": "Determines which compression mode the bucket operates in",
	    "dynamic": true,
            "type": "std::string",
            "validator": {
                "enum": [
                         "off",
                         "passive",
                         "active"
                        ]
            }
        },
        "compaction_max_concurrent_ratio": {
            "default": "0.5",
            "desr" : "Maximum number of CompactVBucketTask tasks which can run concurrently, as a fraction of the possible Writer task concurrency. Note that a minimum of 1, and a maximum of N-1 CompactVBucketTasks will be run (where N is the possible Writer task concurrency), to ensure both forward progress for Compaction and Flushing.",
            "dynamic": true,
            "type" : "float",
            "validator": {
                "range": {
                    "min": 0.0,
                    "max": 1.0
                }
            }
        },
        "concurrent_pagers": {
            "default": "2",
            "descr": "Number of eviction pager tasks to create when memory usage is high",
            "dynamic": false,
            "type": "size_t",
            "validator": {
                "range": {
                    "min": 1
                }
            }
        },
        "expiry_pager_concurrency": {
            "default": "2",
            "descr": "Number of tasks which are created to scan for and delete expired items",
            "dynamic": false,
            "type": "size_t",
            "validator": {
                "range": {
                    "min": 1,
                    "max": 128
                }
            }
        },
        "conflict_resolution_type": {
            "default": "seqno",
            "desr": "Conflict resolution mode to use for this Bucket",
            "dynamic": false,
            "type": "std::string",
            "validator": {
                "enum": [
                    "seqno",
                    "lww",
                    "custom"
                ]
            }
        },
        "couch_bucket": {
            "default": "default",
            "descr": "The name of this bucket",
            "dynamic": false,
            "type": "std::string"
        },
        "data_traffic_enabled": {
            "default": "true",
            "descr": "True if we want to enable data traffic after warmup is complete",
            "dynamic": false,
            "type": "bool"
        },
        "dbname": {
            "default": "",
            "descr": "Path to on-disk storage.",
            "dynamic": false,
            "type": "std::string"
        },
        "dcp_noop_mandatory_for_v5_features": {
            "default": "true",
            "descr": "Forces clients to enable noop for v5 features",
            "dynamic": true,
            "type": "bool"
        },
        "defragmenter_enabled": {
            "default": "true",
            "descr": "True if defragmenter task is enabled",
            "dynamic": true,
            "type": "bool"
        },
        "defragmenter_interval": {
            "default": "10.0",
            "descr": "How often defragmenter task should be run (in seconds).",
            "dynamic": true,
            "type": "float"
        },
        "defragmenter_age_threshold": {
            "default": "1",
            "descr": "How old (measured in number of DefragmenterVisitor passes) must a document be to be considered for defragmentation.",
            "type": "size_t",
            "dynamic" : true
        },
        "defragmenter_stored_value_age_threshold": {
            "default": "1",
            "descr": "How old (measured in number of DefragmenterVisitor passes) must a StoredValue be to be considered for defragmentation.",
            "type": "size_t",
            "dynamic" : true
        },
        "defragmenter_chunk_duration": {
            "default": "20",
            "descr": "Maximum time (in ms) defragmentation task will run for before being paused (and resumed at the next defragmenter_interval).",
            "dynamic": true,
            "type": "size_t",
            "validator": {
                "range": {
                    "min": 1
                }
            }
        },
        "defragmenter_mode" : {
            "default": "auto_pid",
            "descr": "Determines how the defragmenter controls its sleep interval. When static defragmenter_interval is used. When auto_linear, scale the sleep time using a scored defragmentation when it falls between defragmenter_auto_lower_trigger and defragmenter_auto_upper_trigger. When auto_pid use a PID controller to computer reductions in the sleep interval when scored fragmentation is above defragmenter_auto_lower_trigger.",
            "dynamic": true,
            "type": "std::string",
            "validator": {
                "enum": [
                    "static",
                    "auto_linear",
                    "auto_pid"
                ]
            }
        },
        "defragmenter_auto_lower_threshold" : {
            "default": "0.07",
            "descr": "When mode is not static and scored fragmentation is above this value, a sleep time between defragmenter_auto_min_sleep and defragmenter_auto_max_sleep will be used",
            "dynamic": true,
            "type": "float"
        },
        "defragmenter_auto_upper_threshold" : {
            "default": "0.25",
            "descr": "When mode is auto_linear and scored fragmentation is above this value, the defragmenter will use defragmenter_auto_min_sleep",
            "dynamic": true,
            "type": "float"
        },
        "defragmenter_auto_max_sleep" : {
            "default": "10.0",
            "descr": "The maximum sleep that the auto controller can set",
            "dynamic": true,
            "type": "float"
        },
        "defragmenter_auto_min_sleep" : {
            "default": "0.6",
            "descr": "The minimum sleep that the auto controller can set",
            "dynamic": true,
            "type": "float"
        },
        "defragmenter_auto_pid_p" : {
            "default": "52",
            "descr": "The p term for the PID controller",
            "dynamic": true,
            "type": "float"
        },
        "defragmenter_auto_pid_i" : {
            "default": "0.0000197",
            "descr": "The i term for the PID controller",
            "dynamic": true,
            "type": "float"
        },
        "defragmenter_auto_pid_d" : {
            "default": "0.0",
            "descr": "The d term for the PID controller",
            "dynamic": true,
            "type": "float"
        },
        "defragmenter_auto_pid_dt" : {
            "default": "30000",
            "descr": "The dt (interval) term for the PID controller. Value represents milliseconds",
            "dynamic": true,
            "type": "size_t"
        },
        "durability_min_level": {
            "default": "none",
            "descr": "Bucket Minimum Durability Level. KVEngine upgrades any write request to this min-level, if the min-level is higher than the write-level. May upgrade a NormalWrite to SyncWrite.",
            "dynamic": true,
            "type": "std::string",
            "validator": {
                "enum": [
                    "none",
                    "majority",
                    "majority_and_persist_on_master",
                    "persist_to_majority"
                ]
            }
        },
        "ephemeral_full_policy": {
            "default": "auto_delete",
            "descr": "How should an Ephemeral bucket becoming full be handled?",
            "dynamic": true,
            "type": "std::string",
            "validator": {
                "enum": [
                    "auto_delete",
                    "fail_new_data"
                ]
            },
            "requires": {
                "bucket_type": "ephemeral"
            }
        },
        "ephemeral_metadata_purge_age": {
            "default": "60",
            "descr": "Age in seconds after which Ephemeral metadata is purged entirely from memory. Purging disabled if set to -1.",
            "dynamic": true,
            "type": "ssize_t",
            "requires": {
                "bucket_type": "ephemeral"
            }
        },
        "ephemeral_metadata_purge_interval": {
            "default": "60",
            "descr": "Time in seconds between automatic, periodic runs of the Ephemeral metadata purge task. Periodic purging disabled if set to 0.",
            "dynamic": true,
            "type": "size_t",
            "requires": {
                "bucket_type": "ephemeral"
            }
        },
        "ephemeral_metadata_mark_stale_chunk_duration": {
            "default": "20",
            "descr": "Maximum time (in ms) ephemeral hash table cleaner task will run for before being paused (and resumed at the next ephemeral_metadata_purge_interval).",
            "dynamic": true,
            "type": "size_t",
            "requires": {
                "bucket_type": "ephemeral"
            }
        },
        "ephemeral_metadata_purge_stale_chunk_duration": {
            "default": "20",
            "descr": "Maximum time (in ms) ephemeral stale metadata purge task will run for before being paused (and resumed at the next ephemeral_metadata_purge_interval).",
            "dynamic": true,
            "type": "size_t",
            "requires": {
                "bucket_type": "ephemeral"
            }
        },
        "executor_pool_backend": {
            "default": "folly",
            "descr": "Executor Pool backend in use",
            "dynamic": false,
            "type": "std::string",
            "validator": {
                "enum": [
                    "cb3",
                    "folly"
                ]
            }
        },
        "exp_pager_enabled": {
            "default": "true",
            "descr": "True if expiry pager task is enabled",
            "dynamic": true,
            "type": "bool"
        },
        "exp_pager_stime": {
            "default": "600",
            "descr": "Number of seconds between expiry pager runs.",
            "dynamic": true,
            "type": "size_t"
        },
        "exp_pager_initial_run_time": {
            "default": "-1",
            "descr": "Hour in GMT time when expiry pager can be scheduled for initial run",
            "dynamic": true,
            "type": "ssize_t",
            "validator": {
                "range": {
                    "max": 23,
                    "min": -1
                }
            }
        },
        "failpartialwarmup": {
            "default": "true",
            "descr": "If true then do not allow traffic to be enabled to the bucket if warmup didn't complete successfully",
            "dynamic": false,
            "type": "bool"
        },
        "flusher_total_batch_limit" : {
            "default": "4000000",
            "descr": "Number of items that all flushers can be currently flushing. Each flusher has flusher_total_batch_limit / num_writer_threads individual batch size. Individual batches may be larger than this value, as we cannot split Memory checkpoints across multiple commits.",
            "dynamic": true,
            "type": "size_t"
        },
        "flush_batch_max_bytes": {
            "default": "2147483648",
            "descr": "Max size (in bytes) of a single flush-batch passed to the KVStore for persistence.",
            "dynamic": true,
            "type": "size_t",
            "validator": {
                "range": {
                    "min": 1
                }
            }
        },
        "freq_counter_increment_factor": {
            "default": "0.012",
            "descr": "The increment factor of the ProbabilisticCounter being used for the frequency counter. The default value of 0.012 is set such that it allows an 8-bit ProbabilisticCounter to mimic a uint16 counter. See the comment on the ProbabilisticCounter class for more information.",
            "dynamic": false,
            "type": "float",
            "validator": {
                "range": {
                    "min": 0
                }
            }
        },
        "getl_default_timeout": {
            "default": "15",
            "descr": "The default timeout for a getl lock in (s)",
            "dynamic": true,
            "type": "size_t"
        },
        "getl_max_timeout": {
            "default": "30",
            "descr": "The maximum timeout for a getl lock in (s)",
            "dynamic": true,
            "type": "size_t"
        },
        "hlc_drift_ahead_threshold_us": {
            "default": "5000000",
            "descr": "The μs threshold of drift at which we will increment a vbucket's ahead counter.",
            "dynamic": true,
            "type": "size_t"
        },
        "hlc_drift_behind_threshold_us": {
            "default": "5000000",
            "descr": "The μs threshold of drift at which we will increment a vbucket's behind counter.",
            "dynamic": true,
            "type": "size_t"
        },
        "ht_locks": {
            "default": "47",
            "dynamic": false,
            "type": "size_t"
        },
        "ht_resize_interval": {
            "default": "1",
            "descr": "Interval in seconds to wait between HashtableResizerTask executions.",
            "dynamic": true,
            "type": "size_t"
        },
        "ht_size": {
            "default": "47",
            "descr": "The initial and minimum number of slots in HashTable objects.",
            "dynamic": true,
            "type": "size_t"
        },
        "item_compressor_interval": {
            "default": "250",
            "descr": "How often the item compressor task should run (in milliseconds)",
            "dynamic": true,
            "type": "size_t"
        },
        "item_compressor_chunk_duration": {
            "default": "20",
            "descr": "Maximum time (in ms) item compression task will run for before being paused (and resumed at the next item_compressor_interval).",
            "dynamic": true,
            "type": "size_t",
            "validator": {
                "range": {
                    "min": 1
                }
            }
        },
        "item_eviction_policy": {
            "default": "value_only",
            "descr": "Item eviction policy on cache, which is used by the item pager",
            "dynamic": false,
            "type": "std::string",
            "validator": {
                "enum": [
                    "value_only",
                    "full_eviction"
                ]
            },
            "requires": {
                "bucket_type": "persistent"
            }
        },
        "item_eviction_age_percentage": {
            "default": "30",
            "descr": "The age percentage used when determining the age threshold in the learning_age_and_mfu eviction policy.",
            "dynamic": true,
            "type": "size_t",
            "validator": {
                "range": {
                    "min": 0,
                    "max": 100
                }
            }
        },
        "item_eviction_freq_counter_age_threshold": {
            "default": "1",
            "decr": "The threshold for determining at what execution frequency we consider age when selecting items for eviction.",
            "dynamic": true,
            "type": "size_t",
            "validator": {
                "range": {
                    "min": 0,
                    "max": 255
                }
            }
        },
        "item_eviction_strategy": {
            "default": "upfront_mfu_only",
            "descr": "Strategy used by eviction to select items to evict. May be based solely on item hotness (upfront_mfu_only) or may learn the distribution of age and hotness (learning_age_and_mfu). In the case of learning_age_and_mfu, the initial item hotness value is always fixed regardless of the initial_mfu_percentile setting.",
            "dynamic": true,
            "type": "std::string",
            "validator": {
                "enum": [
                    "upfront_mfu_only",
                    "learning_age_and_mfu"
                ]
            }
        },
        "item_eviction_initial_mfu_percentile": {
            "default": "25",
            "descr": "Percentile of existing item MFU distribution to use to determine the MFU to give to new items (0 would insert new items with MFU equal to that of the coldest item present; 100 to that of hottest item present) for the upfront_mfu_only eviction strategy.",
            "dynamic": true,
            "type": "size_t",
            "validator": {
                "range": {
                    "min": 0,
                    "max": 100
                }
            }
        },
        "item_eviction_initial_mfu_update_interval": {
            "default": "1",
            "descr": "Time between updates of the initial MFU given to new items (seconds)",
            "dynamic": true,
            "type": "float",
            "validator": {
                "range": {
                    "min": 1
                }
            }
        },
        "item_freq_decayer_chunk_duration": {
            "default": "20",
            "descr": "Maximum time (in ms) itemFreqDecayer task will run for before being paused.",
            "dynamic": true,
            "type": "size_t",
            "validator": {
                "range": {
                    "min": 1
                }
            }
        },
        "item_freq_decayer_percent": {
            "default": "50",
            "descr": "The percent that the frequency counter of a document is decayed when visited by item_freq_decayer.",
            "dynamic": true,
            "type": "size_t",
            "validator": {
                "range": {
                    "min": 0,
                    "max": 100
                }
            }
        },
        "connection_manager_interval": {
            "default": "0.1",
            "descr": "How often connection manager task should be run (in seconds).",
            "type": "float",
            "dynamic": true,
            "validator": {
                "range": {
                    "min": 0.1
                }
            }
        },
        "connection_cleanup_interval": {
            "default": "1",
            "descr": "How often connection manager task should release dead connections (in seconds).",
            "type": "float",
            "dynamic": true,
            "validator": {
                "range": {
                    "min": 0.1
                }
            }
        },
        "max_checkpoints": {
            "default": "10",
            "descr": "The expected max number of checkpoints in each VBucket on a balanced system. Note: That is not a hard limit on the single vbucket. That is used (together with checkpoint_memory_ratio) for computing checkpoint_max_size, which triggers checkpoint creation.",
            "dynamic": true,
            "type": "size_t",
            "validator": {
                "range": {
                    "min": 2
                }
            }
        },
        "max_failover_entries": {
            "default": "25",
            "descr": "maximum number of failover log entries",
            "dynamic": false,
            "type": "size_t"
        },
        "max_item_privileged_bytes": {
            "default": "(1024 * 1024)",
            "descr": "Maximum number of bytes allowed for 'privileged' (system) data for an item in addition to the max_item_size bytes",
            "dynamic": true,
            "type": "size_t"
        },
        "max_item_size": {
            "default": "(20 * 1024 * 1024)",
            "descr": "Maximum number of bytes allowed for an item",
            "dynamic": true,
            "type": "size_t"
        },
        "max_size": {
            "default": "(100 * 1024 * 1024)",
            "dynamic": true,
            "type": "size_t",
            "descr": "Memory quota (in bytes) for this bucket.",
            "aliases":["cache_size"]
        },
        "max_ttl":{
            "default": "0",
            "descr": "A maximum TTL (in seconds) that will apply to all new documents, documents set with no TTL will be given this value. A value of 0 means this is disabled",
            "dynamic": true,
            "type": "size_t",
            "validator": {
                "range": {
                    "min": 0,
                    "max": 2147483647
                }
            }
        },
        "max_vbuckets": {
            "default": {
                "on-prem": "1024",
                "serverless": "48"
            },
            "descr": "Maximum number of vbuckets expected",
            "dynamic": false,
            "type": "size_t"
        },
        "max_threads": {
            "default": "0",
            "descr": "Maximum number of threads of any single class (0 = automatically select based on core count)",
            "dynamic": false,
            "type": "size_t"
        },
        "max_num_bgfetchers": {
            "default": "0",
            "descr": "Maximum number of bg fetcher objects (the number of concurrent bg fetch tasks we can run). 0 = auto-configure which means we use the same number as the number of shards (max_num_shards - for historic reasons). See also num_reader_threads.",
            "dynamic": false,
            "type": "size_t"
        },
        "max_num_flushers": {
            "default": "0",
            "descr": "Maximum number of flusher objects (the number of concurrent flusher tasks we can run). 0 = auto-configure which means we use the same number as the number of shards (max_num_shards - for historic reasons). See also num_writer_threads.",
            "dynamic": false,
            "type": "size_t"
        },
        "max_num_shards": {
            "default": {
                "on-prem": "0",
                "serverless": "1"
            },
            "descr": "Maximum mumber of shards (0 = auto-configure)",
            "dynamic": false,
            "type": "size_t"
        },
        "max_num_workers": {
            "default": "4",
            "descr": "Bucket Priority relative to other buckets",
            "dynamic": false,
            "type": "size_t",
            "validator": {
                "range": {
                    "max": 8,
                    "min": 1
                }
            }
        },
        "mem_used_merge_threshold_percent" : {
            "default": "0.5",
            "descr": "What percent of max_data size should we allow the estimated total memory to lag by (EPStats::getEstimatedTotalMemoryUsed)",
            "dynamic": true,
            "type": "float",
            "validator": {
                "range": {
                    "max": 100.0,
                    "min": 0.0
                }
            }
        },
        "nexus_primary_backend": {
            "default": "couchdb",
            "descr": "If using the Nexus testing backend then this configuration parameter specifies which type of KVStore to create as the primary variant.",
            "dynamic": false,
            "type": "std::string",
            "validator": {
                "enum": [
                    "couchdb",
                    "magma"
                ]
            },
            "requires": {
                "backend": "nexus"
            }
        },
        "nexus_secondary_backend": {
            "default": "magma",
            "descr": "If using the Nexus testing backend then this configuration parameter specifies which type of KVStore to create as the secondary variant.",
            "dynamic": false,
            "type": "std::string",
            "validator": {
                "enum": [
                    "couchdb",
                    "magma"
                ]
            },
            "requires": {
                "backend": "nexus"
            }
        },
        "nexus_error_handling": {
            "default": "throw",
            "descr": "If using the Nexus testing backend then this config parameter specifies how errors (discrepancies) between the primary and secondary backends are handled.",
            "dynamic": false,
            "type": "std::string",
            "validator": {
                "enum": [
                    "abort",
                    "log",
                    "throw"
                ]
            },
            "requires": {
                "backend": "nexus"
            }
        },
        "nexus_implicit_compaction_enabled": {
            "default": "true",
            "descr": "Should NexusKVStore enable implicit compaction?",
            "dynamic": false,
            "type": "bool",
            "requires": {
                "backend": "nexus"
            }
        },
        "nexus_concurrent_flush_compaction_enabled": {
            "default": "true",
            "descr": "Should NexusKVStore enable concurrent flushing and compaction?",
            "dynamic": false,
            "type": "bool",
            "requires": {
                "backend": "nexus"
            }
        },
        "mem_high_wat": {
            "default": "max",
            "dynamic": true,
            "type": "size_t"
        },
        "mem_low_wat": {
            "default": "max",
            "dynamic": true,
            "type": "size_t"
        },
        "mutation_mem_ratio": {
            "default": "0.93",
            "desr": "Ratio of the Bucket Quota that can be used before mutations return tmpOOMs",
            "dynamic": true,
            "type": "float",
            "validator" : {
                "range" : {
                    "max": 1.0,
                    "min": 0.0
                }
            }
        },
        "pager_sleep_time_ms": {
            "default": "5000",
            "descr": "How long in milliseconds the ItemPager will sleep for when not being requested to run",
            "dynamic": true,
            "type": "size_t"
        },
        "persistent_metadata_purge_age": {
            "default": "259200",
            "descr": "Age in seconds after which tombstones may be purged. Defaults to 3 days. Max of 60 days. If this is dynamically changed for a magma bucket then magma may not trigger compactions when it should, this can be avoided by running a full manual compaction after changing this parameter.",
            "dynamic": true,
            "type": "size_t",
            "requires": {
                "bucket_type": "persistent"
            },
            "validator": {
                "range": {
                    "max": 5184000,
                    "min": 60
                }
            }
        },
        "pitr_enabled" : {
            "default": "false",
            "descr": "Is PiTR enabled or not",
            "dynamic": true,
            "type": "bool"
        },
        "pitr_max_history_age": {
            "default":  "86400",
            "descr": "The number of seconds of the oldest entry to keep as part of compaction (up to 48 hours)",
            "dynamic" : true,
            "type": "size_t",
            "validator": {
                "range": {
                    "max": 172800,
                    "min": 1
                }
            }
        },
        "pitr_granularity": {
            "default":  "600",
            "descr": "The granularity (interval between each rollback point) in seconds (up to 5 hours)",
            "dynamic" : true,
            "type": "size_t",
            "validator": {
                "range": {
                    "max": 18000,
                    "min": 1
                }
            }
        },
        "uuid": {
            "default": "",
            "descr": "The UUID for the bucket",
            "dynamic" : false,
            "type": "std::string"
        },
        "dcp_backfill_byte_limit": {
            "default": {
                "on-prem": "20 * 1024 * 1024",
                "serverless": "512 * 1024"
            },
            "descr": "Max bytes a connection can backfill into memory before backfill is paused",
            "dynamic": true,
            "type": "size_t"
        },
        "dcp_backfill_in_progress_per_connection_limit": {
            "default": "64",
            "descr": "The maximum number of backfills each connection can have in-progress (i.e. KVStore snapshot open and reading data from)",
            "dynamic": true,
            "type": "size_t"
        },
        "dcp_consumer_flow_control_enabled": {
            "default": "true",
            "descr": "Whether DCP Consumer on this node enable flow control",
            "dynamic": false,
            "type": "bool"
        },
        "dcp_consumer_buffer_ratio": {
            "default": "0.05",
            "descr": "Ratio of the BucketQuota that can be allocated by all DCP consumers for buffered messages",
            "type": "float",
            "dynamic": true,
            "validator": {
                "range": {
                    "min": 0.0,
                    "max": 0.2
                }
            }
        },
        "dcp_consumer_flow_control_ack_ratio": {
            "default": "0.2",
            "descr": "Ratio of freed bytes in the DCP Consumer buffer that triggers a BufferAck message to the Producer",
            "type": "float",
            "dynamic": false
        },
        "dcp_consumer_flow_control_ack_seconds": {
            "default": "5",
            "descr": "Max seconds after which a Consumer acks all the remaining freed bytes, regardless of whether dcp_consumer_flow_control_ack_ratio has kicked-in or not",
            "type": "size_t",
            "dynamic": false
        },
        "dcp_enable_noop": {
            "default": "true",
            "descr": "Whether DCP Consumer connections should attempt to negotiate no-ops with the Producer",
            "dynamic": true,
            "type": "bool"
        },
        "dcp_min_compression_ratio": {
            "default": "0.85",
            "desr": "Compression ratio to be achieved above which producer will ship documents as is",
            "dynamic": true,
            "type": "float",
            "validator": {
                "range": {
                    "min": 0.0
                }
            }
        },
        "dcp_idle_timeout": {
            "default": "360",
            "descr": "The maximum number of seconds between dcp messages before a connection is disconnected",
            "dynamic": true,
            "type": "size_t"
        },
        "dcp_noop_tx_interval": {
            "default": "0.1",
            "descr": "The time interval in seconds between noop messages being sent to the consumer",
            "dynamic": true,
            "type": "float",
            "validator": {
                "range": {
                    "max" : 360,
                    "min" : 0.1
                }
            }
        },
        "dcp_oso_backfill": {
            "default": "auto",
            "descr": "Whether DCP out-of-sequence-order backfill is enabled, for backfills which it is applicable. Enabled: always use OSO if possible. Disabled: never use OSO, fallback to seqno order. Auto: use OSO only if it is expected to be faster than seqno.",
            "dynamic": true,
            "type": "std::string",
            "validator": {
                "enum": [
                    "enabled",
                    "disabled",
                    "auto"
                ]
            }
        },
        "dcp_oso_backfill_large_value_ratio": {
            "default": "0.04",
            "desr": "When considering out-of-seqno order (OSO) DCP backfill for a collection with 'large' mean value size, OSO will only be selected if the backfilling collection item count is less than this fraction of the vBucket item count. Whether the collection has 'small' or 'large' items depends on the value of 'dcp_oso_backfill_mean_item_size_threshold'.",
            "dynamic": true,
            "type": "float",
            "validator": {
                "range": {
                    "min": 0.0,
                    "max": 1.0
                }
            }
        },
        "dcp_oso_backfill_small_value_ratio": {
            "default": "0.005",
            "desr": "When considering out-of-seqno order (OSO) DCP backfill for a collection with 'small' mean value size, OSO will only be selected if the backfilling collection item count is less than this fraction of the vBucket item count. Whether the collection has 'small' or 'large' items depends on the value of 'dcp_oso_backfill_mean_item_size_threshold'.",
            "dynamic": true,
            "type": "float",
            "validator": {
                "range": {
                    "min": 0.0,
                    "max": 1.0
                }
            }
        },
        "dcp_oso_backfill_small_item_size_threshold": {
            "default": "256",
            "desr": "When considering out-of-seqno order (OSO) DCP backfill for a collection, should the items in the collection be considered 'small' or 'large' and subsequently should backfill use 'dcp_oso_backfill_small_value_collection_ratio' or 'dcp_oso_backfill_large_value_collection_ratio' when deciding ot use OSO or not? Collections whoss mean on-disk value size is less than this parameter are considered 'small', otherwise they are considered 'large'",
            "dynamic": true,
            "type": "size_t"
        },
        "dcp_oso_max_collections_per_backfill": {
            "default": "1",
            "desr": "This is the maximum number of collections that a DCP stream can be filtering to be eligible for OSO",
            "dynamic": true,
            "type": "size_t"
        },
        "dcp_scan_byte_limit": {
            "default": {
                "on-prem": "4 * 1024 * 1024",
                "serverless": "128 * 1024"
            },
            "descr": "Max bytes that can be read in a single backfill scan before yielding",
            "dynamic": false,
            "type": "size_t"
        },
        "dcp_scan_item_limit": {
            "default": "4096",
            "descr": "Max items that can be read in a single backfill scan before yielding",
            "dynamic": false,
            "type": "size_t"
        },
        "dcp_takeover_max_time": {
            "default": "60",
            "descr": "Max amount of time for takeover send (in seconds) after which front end ops would return ETMPFAIL",
            "dynamic": true,
            "type": "size_t"
        },
        "dcp_producer_snapshot_marker_yield_limit": {
            "default": "10",
            "descr": "The number of snapshots before ActiveStreamCheckpointProcessorTask::run yields.",
            "dynamic": true,
            "type": "size_t",
            "validator": {
                "range": {
                    "max": 100000000,
                    "min": 1
                }
            }
        },
        "dcp_consumer_process_unacked_bytes_yield_limit" : {
            "default": "10",
            "descr": "The number of DcpConsumerTask iterations before forcing the task to yield.",
	        "dynamic": true,
            "type": "size_t",
            "validator": {
                "range": {
                    "max": 100000000,
                    "min": 1
                }
            }
        },
        "fsync_after_every_n_bytes_written": {
            "default": "1048576",
            "descr": "Perform a file sync() operation after every N bytes written. Disabled if set to 0.",
            "dynamic": true,
            "type" : "size_t"
        },
        "min_compression_ratio": {
            "default": "1.2",
            "descr": "specifies a minimum compression ratio below which storing the document will be stored as uncompressed.",
            "dynamic": true,
            "type" : "float",
            "validator": {
                "range": {
                    "min": 0.0
                }
            }
        },
        "magma_per_document_compression_enabled": {
            "default": "false",
            "dynamic": true,
            "descr": "Apply Snappy compression to each document when persisted (magma only)",
            "type": "bool"
        },
        "magma_delete_memtable_writecache": {
            "default": "8192",
            "dynamic": false,
            "descr": "Magma uses a lazy update model to maintain the sequence index. It maintains a list of deleted seq #s that were deleted from the key Index.",
            "type": "size_t"
        },
        "magma_delete_frag_ratio": {
            "default": "0.50",
            "dynamic": false,
            "descr": "Magma compaction always removes duplicate keys but not all sstables are visited during compaction.. This is the minimum fragmentation ratio threshold for when a compaction will be triggerred.",
            "type": "float"
        },
        "magma_max_checkpoints": {
            "default": "5",
            "dynamic": false,
            "descr": "Maximum # of checkpoints retained for rollback.",
            "type": "size_t"
        },
        "magma_checkpoint_interval": {
            "default": "120",
            "dynamic": false,
            "descr": "Frequency of checkpoint interval; in seconds. A checkpoint provides a rollback point to which the data store can rollback to in the event of a failure.",
            "type": "size_t"
        },
        "magma_min_checkpoint_interval": {
            "default": "1",
            "dynamic": false,
            "descr": "Minimum interval between two checkpoints; in seconds. Prevents excessive creation of checkpoints.",
            "type": "size_t"
        },
        "magma_checkpoint_threshold": {
            "default": "0.02",
            "dynamic": false,
            "descr": "Threshold of data written before a checkpoint is created; threshold is based on a fraction of the total data size. Checkpoints require data to be retained in order to provide rollback capability. If the amount of data written during a checkpoint interval is large, we need to do more frequent checkpoints to reduce space amplification.",
            "type": "float"
        },
        "magma_heartbeat_interval": {
            "default": "10",
            "dynamic": false,
            "descr": "Frequency of heartbeat interval; in seconds. A heartbeat task is scheduled to provide cleanup and maintenance when magma is idle.",
            "type": "size_t"
        },
        "magma_write_cache_ratio": {
            "default": "0.2",
            "dynamic": false,
            "descr": "Memory is maintained across 3 magma components; Bloom filters, Block cache and Write cache. The least important of these is the write cache. If there is insufficent memory for the write cache, the write cache will grow to the size of the batch and then be immediately flushed and freed. If there is available memory, the write cache is limited to 20% of the available memory (after bloom filter and block cache get their memory up to magma_max_write_cache (128MB). Bloom filters are the most important and are never paged out. Bloom filter memory can cause magma to go above the memory quota. To allevaite this, the bottom layer where the majority of bloom filter memory is, won't use bloom filters when OptimizeBloomFilterForMisses is on (which it is by default). The block cache grows each time the index sizes change. But its growth is bounded by the available memory or what's left over after the bloom filter memory is subtracted.",
            "type": "float"
        },
        "magma_max_write_cache": {
            "default": "134217728",
            "dynamic": false,
            "descr": "Magma uses a common skiplist to buffer all items at the shard level called the write cache. The write cache contains items from all the kvstores that are part of the shard and when it is flushed, each kvstore will receive a few items each. Regardless of how much memory might be available, this would be the maximum amount that could be allocated.",
            "type": "size_t"
        },
        "magma_mem_quota_ratio": {
            "default": "0.5",
            "dynamic": true,
            "descr": "Magma total memory ratio of the Bucket Quota across all shards and Magma limit's it's memory usage to this value.",
            "type": "float",
            "requires": {
                "bucket_type": "persistent",
                "backend": "magma"
            }
        },
        "magma_mem_quota_low_watermark_ratio": {
            "default": "0.2",
            "dynamic": false,
            "descr": "Fraction of memory quota used by magma as it's low water mark. Magma uses this low watermark to size it's write cache and block cache. This sizing includes bloom filters memory usage but bloom filter eviction is based on the memory quota",
            "type": "float"
        },
        "magma_enable_direct_io": {
            "default": "false",
            "dynamic": false,
            "descr": "Using direct IO tells magma to bypass the file system cache when writing or reading sstables.",
            "type": "bool"
        },
        "magma_initial_wal_buffer_size": {
            "default": "65536",
            "dynamic": false,
            "descr": "The WAL buffer is used to stage items to the write ahead log along with control information like begin and end transaction. This parameter refers to the initial WAL buffer size. The WAL buffer will adjust its size up to a maximum of 4MB or down to a minimum of 64KB depending on the transaction batch size with consideration for other magma components which consume memory such as the block cache, bloom filters, write cache and meta data overhead.",
            "type": "size_t"
        },
        "magma_flusher_thread_percentage": {
            "default": "20",
            "dynamic": true,
            "descr": "Percentage of storage threads that are flusher threads (i.e. with a value of 20 we will allocate 4 (1/5th) of the storage threads to flushers and the remaining 16 (4/5ths) threads will be compactors).",
            "type": "size_t"
        },
        "magma_enable_group_commit": {
            "default": "false",
            "desr": "Group Commit allows transactions in magma to be grouped together to reduce the number of WAL fsyncs. When a transaction is ready to fsync, if there are new transactions waiting to start, we stall the transaction waiting to fsync until there are no more transactions waiting to start for a given magma instance.",
            "dynamic": false,
            "type": "bool"
        },
        "magma_group_commit_max_sync_wait_duration_ms": {
            "default": "20",
            "desr": "When a transaction is about to stall because there are pending transactions waiting to start, if there already are transactions waiting and the oldest transaction has been waiting for magma_group_commit_max_sync_wait_duration ms or more, the current transaction will perform the fsync. When group commit is enabled and both magma_group_commit_max_sync_wait_duration and magma_group_commit_max_transaction_count are set to 0, transactions will stall until there are no more transactions waiting to start. Unit is milliseconds.",
            "dynamic": false,
            "type": "size_t"
        },
        "magma_group_commit_max_transaction_count": {
            "default": "20",
            "desr": "When a transaction is about to stall because there are pending transactions waiting to start, if there already are magma_group_commit_max_transaction_count including the current transaction waiting, the current transaction will perform the fsync. When group commit is enabled and both magma_group_commit_max_sync_wait_duration and magma_group_commit_max_transaction_count are set to 0, transactions will stall until there are no more transactions waiting to start.",
            "dynamic": false,
            "type": "size_t"
        },
        "magma_max_default_storage_threads": {
            "default": "20",
            "dynamic": false,
            "descr": "If the number of storage threads = 0, then we set the number of storage threads based on the number of writer threads up to a maximum of 20 threads and use magma_flusher_thread_percentage to determine the ratio of flusher and compactor threads.",
            "type": "size_t"
        },
        "magma_sync_every_batch": {
            "default": "false",
            "dynamic": false,
            "descr": "Couchstore generates a commit point at the end of every batch of items. During normal operation, Magma checkpoints are taken at every magma_checkpoint_interval. Many of the tests require more frequent checkpoints so this configuration parameter makes sure every batch generates a checkpoint. Each checkpoint generated in this way is a \"Sync\" checkpoint and isn't going to be useful for rollback as it only the latest checkpoint is a \"Sync\" checkpoiont. A \"Rollback\" checkpoint will be made instead if we set magma_checkpoint_interval to 0. The \"Rollback\" checkpoints are stored in the checkpoint queue as potential rollback points. Should be used for testing only!",
            "type": "bool"
        },
        "magma_min_value_block_size_threshold": {
            "default": "(64 * 1024)",
            "dynamic": false,
            "descr": "Magma creates value blocks for values larger than this size. Value blocks only contain a single KV item and their reads/writes are optimised for memory as it avoids many value copies. Right now compression is turned off for value blocks to reduce memory consumption while building them. This setting should be at least as large as the SeqIndex block size.", 
            "type": "size_t"
        },
        "magma_enable_memory_optimized_writes": {
            "default": {
                "on-prem": "false",
                "serverless": "true"
            },
            "dynamic": false,
            "descr": "When enabled, if copying a write batch into memtable results in exceeding the write cache quota, Magma avoids the copy and instead flushes the batch to disk on the writer thread itself. This tradeoffs an increase in write latency for reduced memory consumption and obeys quota limits. If copying a batch keeps us under the quota, Magma will to continue to copy and do the flush in background.",
            "type": "bool"
        },
        "magma_enable_wal": {
            "default": "true",
            "dynamic": false,
            "descr": "WAL ensures Magma's atomicity, durability. Disabling it is useful in performance analysis.",
            "type": "bool"
        },
        "magma_seq_tree_data_block_size": {
            "default": "4096",
            "dynamic": true,
            "descr": "Magma uses SSTables for storage. SSTables are made up of different types of blocks. Data blocks contain the bulk of the data and contain the key, metadata and value for each of the items in the block. Larger block sizes can decrease storage space by better block compression but they require more memory, cpu and io bandwidth to read and write them.",
            "type": "size_t",
            "validator": {
                "range": {
                    "min": 4096,
                    "max": 131072
                }
            },
            "requires": {
                "bucket_type": "persistent",
                "backend": "magma"
            }
        },
        "magma_seq_tree_index_block_size": {
            "default": "4096",
            "dynamic": true,
            "descr": "Magma uses SSTables for storage. SSTables are made up of different types of blocks. Index blocks contain keys that help traverse the SSTable to locate the data item. Larger block sizes can decrease storage space by better block compression but they require more memory, cpu and io bandwidth to read and write them.",
            "type": "size_t",
            "validator": {
                "range": {
                    "min": 4096,
                    "max": 131072
                }
            },
            "requires": {
                "bucket_type": "persistent",
                "backend": "magma"
            }
        },
        "magma_key_tree_data_block_size": {
            "default": "4096",
            "dynamic": true,
            "descr": "Magma uses SSTables for storage. SSTables are made up of different types of blocks. Data blocks contain the bulk of the data and contain the key and metadata for each of the items in the block. Larger block sizes can decrease storage space by better block compression but they require more memory, cpu and io bandwidth to read and write them.",
            "type": "size_t",
            "validator": {
                "range": {
                    "min": 4096,
                    "max": 131072
                }
            },
            "requires": {
                "bucket_type": "persistent",
                "backend": "magma"
            }
        },
        "magma_key_tree_index_block_size": {
            "default": "4096",
            "dynamic": true,
            "descr": "Magma uses SSTables for storage. SSTables are made up of different types of blocks. Index blocks contain keys that help traverse the SSTable to locate the data item. Larger block sizes can decrease storage space by better block compression but they require more memory, cpu and io bandwidth to read and write them.",
            "type": "size_t",
            "validator": {
                "range": {
                    "min": 4096,
                    "max": 131072
                }
            },
            "requires": {
                "bucket_type": "persistent",
                "backend": "magma"
            }
        },
        "magma_enable_upsert": {
            "default": "false",
            "dynamic": false,
            "descr": "When true, the kv_engine will utilize Magma's upsert capabiltiy but accurate document counts for the data store or collections can not be maintained.",
            "type": "bool"
        },
        "magma_expiry_purger_interval": {
            "default": "3600",
            "dynamic":false,
            "descr": "Magma maintains statistics about expired documents to run compaction based on magma_expiry_frag_threshold. This config determines the the expiry purger polling interval in seconds to trigger compaction on eligible sstables",
            "type": "size_t"
        },
        "magma_expiry_frag_threshold": {
            "default": "0.25",
            "dynamic": false,
            "descr": "All compactions perform expiry but not all sstables are visited by compaction. Magma maintains an expiry histogram across the kvstore to help determine which range of sstables need to have compaction run on them because there are a significant number of expired items. The frag threshold is the number of expired keys vs keys in the data store.",
            "type": "float"
        },
        "magma_enable_block_cache": {
            "default": "true",
            "dynamic": true,
            "descr": "The block cache is an LRU policy driven cache that is used to maintain index blocks for the sstable's btrees.",
            "type": "bool"
        },
        "magma_fragmentation_percentage": {
            "default": "50",
            "dynamic": true,
            "descr": "The percentage of fragmentation a magma bucket aims to maintain. A 100 value will disable sequence tree compactions by setting the desired fragmentation percentage to 100%. Smaller compactions of the key and local indexes will still run.",
            "type": "size_t",
            "validator": {
                "range": {
                    "min": 10,
                    "max": 100
                }
            },
            "requires": {
                "bucket_type": "persistent",
                "backend": "magma"
            }
        },
        "magma_max_recovery_bytes": {
            "default": "67108864",
            "dynamic": false,
            "descr": "Maximum amount of data that is replayed from the WAL during magma recovery. When this threshold is reached magma, creates a temporary checkpoint to recover at. This is per kvstore and in bytes.",
            "type": "size_t"
        },
        "magma_max_level_0_ttl": {
            "default": "600",
            "dynamic": false,
            "descr": "Maximum time (in seconds) that data is kept in level 0 before it is merged.",
            "type": "size_t"
        },
        "magma_bloom_filter_accuracy": {
            "default": "0.99",
            "desr": "Magma maintains a bloom filter per sstable in the LSMTree. The bloom filters are used to reduce IO in case of non-existent  This config sets the accuracy of the bloom filters ie. (1 - accuracy) = false positive rate",
            "dynamic": false,
            "type": "float"
        },
        "magma_bloom_filter_accuracy_for_bottom_level": {
            "default": "0.95",
            "desr": "The bloom filters at the bottom level are used to avoid IO in case of non-existent keys. Also most of the data resides in the bottom level. This config allows for lowering of bloom filter accuracy of sstables residing in the lowermost level of the LSMTree.",
            "dynamic": false,
            "type": "float"
        },
        "magma_index_compression_algo": {
            "default": "LZ4",
            "descr": "Compression algorithm used by Magma which is used to compress blocks that don't contain documents ie. all index related blocks",
            "dynamic": false,
            "type": "std::string",
            "validator": {
                "enum": [
                    "LZ4",
                    "Snappy",
                    "Zstd"
                ]
            }
        },
        "magma_data_compression_algo": {
            "default": "LZ4",
            "descr": "Compression algorithm used by Magma to compress data blocks where documents are stored.",
            "dynamic": false,
            "type": "std::string",
            "validator": {
                "enum": [
                    "LZ4",
                    "Snappy",
                    "Zstd"
                ]
            }
        },
        "range_scan_kv_store_scan_ratio": {
            "default": "0.8",
            "dynamic": false,
            "descr": "The ratio for calculating how many RangeScans can exist, a ratio of total KVStore scans.",
            "type": "float",
            "validator": {
                "range": {
                    "min": 0.0,
                    "max": 1.0
                }
            }
        },
        "range_scan_max_continue_tasks": {
            "default": "0",
            "dynamic": true,
            "descr": "The maximum number of range scan tasks that can exist concurrently. Setting to 0 results in num_auxio_threads - 1 tasks",
            "type": "size_t"
        },
        "range_scan_max_lifetime": {
            "default": "180",
            "dynamic": true,
            "descr": "The maximum lifetime in seconds for a range-scan. Scans that don't complete before this limit are cancelled",
            "type": "size_t"
        },
        "range_scan_read_buffer_send_size": {
            "default": "8192",
            "dynamic": true,
            "descr": "The size of a buffer used to store data read during the I/O phase of a range-scan-continue. Once the buffer size is >= to this value the data is sent to the connection",
            "type": "size_t",
            "validator": {
                "range": {
                    "min": 0,
                    "max": 268435456
                }
            }
        },
        "retain_erroneous_tombstones": {
            "default": "true",
            "descr": "whether erroneous tombstones need to be retain during compaction. Erroneous tombstones are those that have invalid meta data in it. For example, a delete time of 0.",
	        "dynamic": true,
            "type": "bool"
        },
        "seqno_persistence_timeout": {
            "default": "30",
            "descr": "Timeout in seconds after which a pending SeqnoPersistence operation is temp-failed",
            "dynamic": true,
            "type": "size_t",
            "validator": {
                "range": {
                    "min": 0,
                    "max": 30
                }
            }
        },
        "sync_writes_max_allowed_replicas" : {
            "default": "2",
            "descr": "The maximum number of supported replicas for SyncWrites. Attempts to issue SyncWrites against a topology with more replicas than this setting will fail with DurabilityImpossible.",
            "dynamic": false,
            "type": "size_t"
        },
        "time_synchronization": {
            "default": "disabled",
            "descr": "No longer supported. This config parameter has no effect.",
            "dynamic": false,
            "type": "std::string",
            "validator": {
                "enum": [
                    "disabled",
                    "enabled_without_drift",
                    "enabled_with_drift"
                ]
            }
        },
        "cross_bucket_ht_quota_sharing": {
          "default": {
            "on-prem": "false",
            "serverless": "false"
          },
          "descr": "Allow this Bucket's HashTable quota to be shared with other Buckets which have this setting enabled.",
          "dynamic": false,
          "type": "bool"
        },
        "couchstore_tracing": {
            "default": "false",
            "dynamic": true,
            "descr": "Enable couchstore tracing",
            "type" : "bool"
        },
        "couchstore_write_validation": {
            "default": "false",
            "dynamic": true,
            "descr": "Validate couchstore writes ",
            "type" : "bool"
        },
        "couchstore_mprotect": {
            "default": "false",
            "dynamic": true,
            "descr": "Enable couchstore to mprotect the iobuffer",
            "type" : "bool"
        },
        "couchstore_file_cache_max_size": {
            "default": "30720",
            "dynamic": true,
            "descr": "Maximum number of couchstore files that we will keep open. Default value is 30 * 1024 (i.e. one file for each vBucket and 30 Buckets - the supported limit).",
            "type": "size_t"
        },
        "couchstore_midpoint_rollback_optimisation": {
            "default": "true",
            "dynamic": false,
            "descr": "Should we have to rollback more than half of the seqnos seen by this vBucket we will instead rollback to 0 and re-stream from the active if set to true",
            "type": "bool"
        },
        "vbucket_mapping_sanity_checking": {
            "default": "false",
            "dynamic": true,
            "descr": "Are vBucket mappings (key -> vBucket) checked by the server? This is a sanity checking mode which crc32 hashes the key to ensure that the client is supplying the expected vBucket for each key.",
            "type": "bool"
        },
        "vbucket_mapping_sanity_checking_error_mode": {
            "default": "log",
            "dynamic": true,
            "descr": "See vbucket_mapping_sanity_checking. This parameter governs how found errors are dealt with. Were we to recommend these sanity checks to a customer then a log or throw handling would likely be most appropriate, for in-house testing an abort on error is far more useful as it is immediately noticeable in test frameworks and provides the maximum amount of information.",
            "type": "std::string",
            "validator": {
                "enum": [
                    "abort",
                    "throw",
                    "log"
                ]
            }
        },
        "history_retention_seconds": {
            "default": "0",
            "dynamic": true,
            "descr": "Seconds of history the bucket should aim to retain on disk.",
            "type": "size_t",
            "requires": {
                "bucket_type": "persistent",
                "backend": "magma"
            }
        },
        "history_retention_bytes": {
            "default": "0",
            "dynamic": true,
            "descr": "Max bytes of history a bucket should aim to retain on disk.",
            "type": "size_t",
            "requires": {
                "bucket_type": "persistent",
                "backend": "magma"
            }
        },
        "warmup": {
            "default": "true",
            "dynamic": false,
            "descr": "Is Warmup of existing data enabled",
            "type": "bool",
            "requires": {
                "bucket_type": "persistent"
            }
        },
        "warmup_batch_size": {
            "default": "10000",
            "descr": "The size of each batch loaded during warmup.",
            "dynamic": false,
            "type": "size_t",
            "validator": {
                "range": {
                    "max": 100000000,
                    "min": 1
                }
            }
        },
        "warmup_backfill_scan_chunk_duration": {
            "default": "100",
            "descr": "The duration (in ms) after which warmup's backfill scans will yield and re-schedule; allowing other tasks on the same threads to run.",
            "dynamic": false,
            "type": "size_t",
            "requires": {
                "bucket_type": "persistent"
            },
            "comment": "The value selected needs to balance throughput of the task itself, and scheduling latency of other tasks on thr same thread. Too high a value (or not yielding at all) means that backfill tasks monopolise the thread they are running on and hence prevent other reader tasks - such as populating vBucket map for other Buckets being warmed up (see MB-47267 / MB-52383). Too low a value and the throughput of the task itself is low and warmup takes a long time - for example functional testing showed that time-to-first-item during a scan may be 40ms on mid-low end disks. If we selected a value less than 40ms then only a single item will be read per scan; given the setup of the scan itself can be costly we want aim to read at least a few items per scan."
        },
        "warmup_min_memory_threshold": {
            "default": {
                "on-prem": "100",
                "serverless": "0"
            },
            "descr": "Percentage of max mem warmed up before we enable traffic.",
            "dynamic": true,
            "type": "size_t",
            "validator": {
                "range": {
                    "max": 100,
                    "min": 0
                }
            }
        },
        "warmup_min_items_threshold": {
            "default": {
                "on-prem": "100",
                "serverless": "0"
            },
            "descr": "Percentage of total items warmed up before we enable traffic.",
            "dynamic": true,
            "type": "size_t",
            "validator": {
                "range": {
                    "max": 100,
                    "min": 0
                }
            }
        },
        "xattr_enabled": {
            "default": "true",
            "dynamic": true,
            "type": "bool"
        }
    }
}
