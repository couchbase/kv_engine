#!/usr/bin/env python3

#   Copyright 2018-Present Couchbase, Inc.
#
#   Use of this software is governed by the Business Source License included
#   in the file licenses/BSL-Couchbase.txt.  As of the Change Date specified
#   in that file, in accordance with the Business Source License, use of this
#   software will be governed by the Apache License, Version 2.0, included in
#   the file licenses/APL2.txt.
#

"""Parses a memcached log file for 'Slow op', 'Slow runtime' and
'Slow scheduling' warnings, and converts into a Google Trace Event Format file
(https://docs.google.com/document/d/1CvAClvFfyA5R-PhYUmn5OOQtYMH4h6I0nSsKchNAySU/preview#heading=h.yr4qxyxotyw).

Usage:
    cat memcached.log | kv_slow_ops_2_gtrace > trace.json
    <open Chrome -> chrome://tracing -> Load 'trace.json'
"""

import argparse
from datetime import datetime, timedelta
import copy
import json
import re
import unittest
import sys


first = True
system_clock_steady_clock_delta = None

# Dictionary tracking which threads have already been 'seen' at least once,
# so we only emit their metadata once.
thread_seen = dict()


def iso8601_to_epoch(ts):
    """Convert an ISO 8601 time string to seconds since unix epoch."""
    return datetime.strptime(ts, '%Y-%m-%dT%H:%M:%S.%f%z').timestamp()


def human_duration_to_sec(h):
    """Converts a human-readable duration to floating-point seconds.
    Supports multiple units, with either whitespace separation or not with
    colon between units or not, e.g.
        123 ms -> 0.123
        2 s -> 2
        3.456s -> 3.456
        10m2.222s -> 602.222
        10m:2s -> 602
    """
    ts = timedelta()
    debug = False
    while True:
        m = re.search(r'^([\d.]+)\s?([a-z]+):?(.*$)', h)
        if not m:
            return ts.total_seconds()
        (value, unit, h) = m.groups()
        if unit == "h":
            ts = ts + timedelta(hours=float(value))
        elif unit == "m":
            ts = ts + timedelta(minutes=float(value))
        elif unit == "s":
            ts = ts + timedelta(seconds=float(value))
        elif unit == "ms":
            ts = ts + timedelta(milliseconds=float(value))
        elif unit == "us":
            ts = ts + timedelta(microseconds=float(value))
        elif unit == "ns":
            ts = ts + timedelta(microseconds=float(value) / 1000)
        else:
            raise ValueError("Unhandled unit '{}'".format(unit))


def update_steady_clock_offset(system_clock, steady_clock):
    """To be able to plot Slow task runtimes/scheduling
    (see parse_slow_task_runtime) on the same timeline as Slow operations, we
    need to map the system-clock timestamp to the steady-clock timestamp used
    by Slow operations. In theory system clock can drift relative to
    steady-clock, so re-calculate the delta for each "Slow operation"
    encountered to minimise any inaccuracies.
    """
    global system_clock_steady_clock_delta
    system_epoch = iso8601_to_epoch(system_clock)
    system_clock_steady_clock_delta = steady_clock - system_epoch


def print_event(event):
    """Converts the event to a JSON object and prints to stdout, adding commas
    between each object to make a JSON list.
    """
    global first
    if not first:
        print(',', end=' ')
    first = False
    print(json.dumps(event))


def parse_slow_operation(node, m):
    """Parses a log line for a 'Slow operation', outputting to stdout a Google
     Chrome tracing event for each span parsed.
    """
    time = m.group(1)
    fd = m.group(2)
    slow_op = parse_slow_op(m.group(3))

    # Set the common fields for all events for this operation
    common = dict()
    common['cat'] = slow_op['command']
    common['ph'] = 'X'
    # clientID "should" be a tuple of "connectionID/opaque", but
    # not all clients obey this convention.
    # As such, we extract the opaque here "just in case" - as older
    # server releases didn't report the full request packet, but if we
    # do have packet.opaque then use that in preference.
    cid = slow_op['cid']
    (connection_id, opaque) = cid.rsplit('/', 1)
    opaque = int(opaque, 16)
    if 'opaque' in slow_op['packet']:
        opaque = slow_op['packet']['opaque']
    common['pid'] = connection_id

    # Build a trace event from each span in the slow op.
    for span in slow_op['trace'].split():
        (name, value) = span.split('=')
        (start_ns, dur) = map(int, value.split(':'))
        # MB-43617: Ignore any spans which have zero time and duration;
        # they are spurious due to missing copy-elision when recording
        # Spans.
        if not (start_ns or dur):
            continue
        event = dict()
        event.update(common)
        event['name'] = name
        # Timestamp and duration need to be in microseconds.
        event['ts'] = start_ns / 1000
        event['dur'] = dur
        # For the top-level 'request' event, include additional request
        # details (redundant to repeat for every event).
        if name == 'request':
            event['args'] = {'opaque': opaque,
                             'fd': fd,
                             'peer': slow_op['peer']}
            try:
                event['args']['key'] = slow_op['packet']['key']
            except KeyError:
                pass
            event['args']['bucket'] = slow_op['bucket']
            event['args']['vbucket'] = slow_op['packet']['vbucket']
            event['args']['time'] = time
            # worker_tid added in v7.1.0.
            if 'worker_tid' in slow_op:
                event['args']['worker_tid'] = slow_op['worker_tid']

            # Calculate end of event in seconds according to steady_clock,
            # then update clock offset.
            ts_end_sec = (event['ts'] + event['dur']) / 1000000
            update_steady_clock_offset(time, ts_end_sec)
        if node:
            if 'args' not in event:
                event['args'] = dict()
            event['args']['node'] = node
        print_event(event)


def parse_slow_task_runtime(m):
    (time, bucket, task, thread, runtime) = m.groups()

    event = dict()
    event['cat'] = "Slow runtime"
    event['name'] = task
    event['ph'] = "X"
    event['pid'] = thread

    # Convert log message timestamp to same time domain as Slow operations
    # (steady_clock seconds)
    timestamp_epoch = iso8601_to_epoch(time)
    timestamp_steady_clock = timestamp_epoch + system_clock_steady_clock_delta
    runtime_sec = human_duration_to_sec(runtime)

    # Timestamps and duration are expressed in microseconds
    event['ts'] = (timestamp_steady_clock - runtime_sec) * 1000000
    event['dur'] = runtime_sec * 1000000
    event['args'] = {'time': time}
    print_event(event)

    if thread not in thread_seen:
        meta = dict()
        meta['name'] = "process_sort_index"
        meta['ph'] = "M"
        meta['pid'] = thread
        meta['args'] = {'sort_index': -1}
        print_event(meta)
        thread_seen[thread] = True


def parse_slow_task_scheduling(m):
    (time, bucket, task, thread, runtime) = m.groups()

    # 'Slow scheduling' events are asynchronous with respect to the reported
    # thread - i.e. there could be multiple tasks which all have slow scheduling
    # at the same time, and hence conceptually overlap each other. As such,
    # we represent as Async Google Trace events, which require (b)egin and
    # (e)end events.
    event = dict()
    event['args'] = {'time': time, 'task': task}
    event['cat'] = "Slow scheduling"
    event['name'] = "Slow scheduling"
    event['id'] = task + time
    event['pid'] = thread

    # Convert log message timestamp to same time domain as Slow operations
    # (steady_clock seconds)
    timestamp_epoch = iso8601_to_epoch(time)
    timestamp_steady_clock = timestamp_epoch + system_clock_steady_clock_delta
    runtime_sec = human_duration_to_sec(runtime)

    begin = copy.deepcopy(event)
    begin['ph'] = "b"

    end = copy.deepcopy(event)
    end['ph'] = "e"

    # Timestamps and duration are expressed in microseconds
    begin['ts'] = (timestamp_steady_clock - runtime_sec) * 1000000
    end['ts'] = begin['ts'] + (runtime_sec * 1000000)

    print_event(begin)
    print_event(end)

    if thread not in thread_seen:
        meta = dict()
        meta['name'] = "process_sort_index"
        meta['ph'] = "M"
        meta['pid'] = thread
        meta['args'] = {'sort_index': -1}
        print_event(meta)
        thread_seen[thread] = True


def parse_slow_op(trace):
    """
    Parse the trace JSON from the slow operation log.
    """

    # In some cases, we might have improperly encoded values in the output.
    # These values are JSON objects wrapped in quotes, but the strings inside
    # the objects are unescaped and cause deserialisation to fail.

    # The cid object with IPv4, potentially with port number partially /
    # entirely omitted, and potentially without closing JSON object '}'.
    trace = re.sub('"{"ip":"([\\d\\.]+)","port":(\\d*)}?\\/(\\w+)"',
                   '"\\1:\\2/\\3"', trace)
    # The cid object with IPv6.
    # This can be quite corrupted:
    # - potentially with only part of IPv6 address
    # - potentially with only part of / none of port
    # - potentially without closing JSON object '}'
    trace = re.sub(
        '"{"ip":"([0-9a-fA-F\\.:]+)"?,?"?p?o?r?t?"?:?(\\d*)}?\\/(\\w+)"',
        '"[\\1]:\\2/\\3"',
        trace)
    # Peer object with IPv4
    trace = re.sub('"{"ip":"([\\d\\.]+)","port":(\\d+)}"',
                   '"\\1:\\2"', trace)
    # Peer object with IPv6
    trace = re.sub('"{"ip":"([0-9a-fA-F\\.:]+)","port":(\\d+)}"',
                   '"[\\1]:\\2"', trace)

    return json.loads(trace)


def main(file):
    print('[')
    global first
    node = ""
    skipped_slow_runtime_events = 0
    skipped_slow_scheduling_events = 0
    for line in file:
        # Check if we're parsing logs from multiple nodes, extracting data for rebasing timestamps if we are.
        #
        # Match strings beginning with a cbcollect filepath, such as:
        # cbcollect_info_ns_1@ip-or-hostname_20220722-111444/memcached.log:[log line from file...]
        # and capture ip-or-hostname to uniquely identify each node.
        if node or first:
            m = re.search("^cbcollect_.*@(.+?)_.*", line)
            if m:
                node = m.group(1)
        # Match ISO 8601 formatted log timestamps. Example:
        # 2022-07-20T01:58:01.316016+03:00
        time_regex = "(\\d{4}-\\d{2}-\\d{2}T[0-9:.+-]+)"
        m = re.search(
            time_regex +
            " WARNING (\\d+): Slow operation. (.*)",
            line)
        if m:
            parse_slow_operation(node, m)
            continue

        # Check for "Slow runtime" messages from tasks, e.g.
        # 2022-11-21T18:06:56.188574-05:00 WARNING (No Engine) Slow runtime for
        # 'Compact DB file 421' on thread WriterPool3: 32 s
        m = re.search(
            time_regex +
            " WARNING \\(([^)]+)\\) Slow runtime for '(.+)' on thread (.+): (.+)",
            line)
        if m:
            if system_clock_steady_clock_delta:
                parse_slow_task_runtime(m)
            else:
                skipped_slow_runtime_events += 1

        # Check for "Slow scheduling" messages from tasks, e.g.
        # 2023-09-18T15:50:39.002619+00:00 WARNING (No Engine) Slow scheduling
        # for NON_IO task 'Paging out items.' on thread NonIoPool1. Schedule
        # overhead: 15 s
        m = re.search(
            time_regex +
            " WARNING \\(([^)]+)\\) Slow scheduling for [\\w_]+ task '(.+)' on thread (.+). Schedule overhead: (.+)",
            line)
        if m:
            if system_clock_steady_clock_delta:
                parse_slow_task_scheduling(m)
            else:
                skipped_slow_scheduling_events += 1
    print(']')
    if skipped_slow_runtime_events > 0 or skipped_slow_scheduling_events > 0:
        print(("Warning: Failed to generate traces for {} 'Slow runtime' and "
               "{} 'Slow scheduling' events as system_clock delta is unset.\n"
               "Ensure there is at least one 'Slow operation' parsed prior to "
               "the Slow runtime / schduler event(s) if you want to include "
               "them.").format(
            skipped_slow_runtime_events,
            skipped_slow_scheduling_events),
            file=sys.stderr)


##
# ---------------------- Unit tests ------------------------------------------
##

class TestParsing(unittest.TestCase):

    def test_6_6_2(self):
        test_string = (
            r'{"cid":"7E/FF/20","duration":"11 s","trace":"request=63:11 '
            r'get=0:0 bg.wait=6308229496700500:55 get=6308229496693730:41 '
            r'bg.load=6308229496755836:11232001 get=0:0 get=630824072879295:11"'
            r',"command":"GET","peer":"{"ip":"0.0.0.0","port":57324}","bucket":'
            r'"b","packet":{"bodylen":49,"cas":0,"datatype":"raw","extlen":0,"'
            r'key":"<ud>c0ca65a1438baaab6c8a156b05d3b08d91bfacd7</ud>",'
            r'"keylen":49,"magic":"ClientRequest","opaque":513417216,'
            r'"opcode":"GET","vbucket":318}}')
        obj = parse_slow_op(test_string)
        self.assertEqual(obj['cid'], '7E/FF/20')
        self.assertEqual(obj['peer'], '0.0.0.0:57324')

    def test_6_6_5(self):
        test_string = (
            r'{"cid":"{"ip":"0.0.0.0","port":42678}/ef","duration":"521 ms",'
            r'"trace":"request=6070831171407831:521890","command":"STAT",'
            r'"peer":"{"ip":"0.0.0.0","port":42678}","bucket":"a","packet":'
            r'{"bodylen":3,"cas":0,"datatype":"raw","extlen":0,"key":'
            r'"<ud>dcp</ud>","keylen":3,"magic":"ClientRequest",'
            r'"opaque":3724742144,"opcode":"STAT","vbucket":0}}'
        )
        obj = parse_slow_op(test_string)
        self.assertEqual(obj['cid'], '0.0.0.0:42678/ef')
        self.assertEqual(obj['peer'], '0.0.0.0:42678')

    def test_6_6_5_capped_connId(self):
        """In 6.6.5, if the client doesn't specify a connectionId via HELO, KV
        generates a JSON from IP and port. However, KV only stores the first
        34 bytes of the connectionId - so the JSON object can get cropped.
        Worst case is an IP where each octet is >=100 and a 5 digit port
        is 40 characters (including nul-termination):

            {"ip": "111.222.333.444", "port":55555}
                                             XXXXXX

        We can therefore lose up to 6 bytes, losing a variable amount from
        after "port:"  (marked XXX above).
        """
        test_string = (
            r'{"cid":"{"ip":"10.149.225.86","port":4541/0","duration":"82 s",'
            r'"trace":"request=363957733255087:82601665","command":'
            r'"SASL_AUTH","peer":"{"ip":"10.149.225.86","port":45418}",'
            r'"bucket":"","packet":{"bodylen":44,"cas":0,"datatype":"raw",'
            r'"extlen":0,"key":"<ud>PLAIN</ud>","keylen":5,"magic":'
            r'"ClientRequest","opaque":0,"opcode":"SASL_AUTH","vbucket":0}}')
        obj = parse_slow_op(test_string)
        self.assertEqual(obj['cid'], '10.149.225.86:4541/0')

    def test_6_6_5_capped_connId_IPv6(self):
        """Similar to test_6_6_5_capped_connId, but with IPv6 addresses, which
        can have even worse cropping. Worst case is a full 128bit address with
        no leading zeros omitted, and 5 digit port which is 62 bytes (including
        nul-termination):

            {"ip":"1111:2222:3333:4444:5555:6666:7777:8888","port":55555}
                                             XXXXXXXXXXXXXXXXXXXXXXXXXXXX

        We can therefore lose up to 28 bytes - part of the IPv6 address and
        potentially all of the port (marked XXX above).
        """
        test_string = (
            r'{"cid":"{"ip":"2001:f547:23:4705::e156","/0","duration":'
            r'"12m:0s","trace":"request=398965059065164:720026408","command":'
            r'"SASL_AUTH","peer":"{"ip":"2001:f547:23:4705::e156","port":'
            r'55610}","bucket":"","packet":{"bodylen":46,"cas":0,"datatype":'
            r'"raw","extlen":0,"key":"<ud>PLAIN</ud>","keylen":5,"magic":'
            r'"ClientRequest","opaque":0,"opcode":"SASL_AUTH","vbucket":0}}')
        obj = parse_slow_op(test_string)
        self.assertEqual(obj['cid'], '[2001:f547:23:4705::e156]:/0')

    def test_6_6_5_IPv6(self):
        test_string = (
            r'{"cid":"{"ip":"684D:1111:222:3333:4444:5555:6:77","port":'
            r'42678}/ef","duration":"521 ms","trace":'
            r'"request=6070831171407831:521890","command":"STAT","peer":"{"ip":'
            r'"684D:1111:222:3333:4444:5555:6:77","port":42678}","bucket":"a",'
            r'"packet":{"bodylen":3,"cas":0,"datatype":"raw","extlen":0,"key":'
            r'"<ud>dcp</ud>","keylen":3,"magic":"ClientRequest",'
            r'"opaque":3724742144,"opcode":"STAT","vbucket":0}}')
        obj = parse_slow_op(test_string)
        self.assertEqual(obj['cid'],
                         '[684D:1111:222:3333:4444:5555:6:77]:42678/ef')
        self.assertEqual(obj['peer'],
                         '[684D:1111:222:3333:4444:5555:6:77]:42678')

    def test_7_2_0(self):
        test_string = (
            r'{"bucket":"c","cid":"AF12/4FC/6c","command":"GET_CLUSTER_CONFIG",'
            r'"connection":"[ {\"ip\":\"0.0.0.0\",\"port\":54300} - '
            r'{\"ip\":\"0.0.0.0\",\"port\":11207} (<ud>dfs</ud>) ]","duration":'
            r'"580 ms","packet":{"bodylen":0,"cas":0,"datatype":"raw",'
            r'"extlen":0,"key":"<ud></ud>","keylen":0,"magic":"ClientRequest",'
            r'"opaque":693831680,"opcode":"GET_CLUSTER_CONFIG","vbucket":0},'
            r'"peer":{"ip":"0.0.0.0","port":54300},"response":"Success",'
            r'"trace":"request=10811483103351:580237 execute=108120633258:15",'
            r'"worker_tid":281472829188416}')
        obj = parse_slow_op(test_string)
        self.assertEqual(obj['cid'], 'AF12/4FC/6c')
        self.assertEqual(obj['peer']['ip'], '0.0.0.0')
        self.assertEqual(obj['peer']['port'], 54300)


class TestHumanDurationToSec(unittest.TestCase):

    def test_milliseconds(self):
        self.assertEqual(human_duration_to_sec("12ms"), 0.012)
        self.assertEqual(human_duration_to_sec("12 ms"), 0.012)

    def test_seconds(self):
        self.assertEqual(human_duration_to_sec("12s"), 12)
        self.assertEqual(human_duration_to_sec("12 s"), 12)

    def test_minutes(self):
        self.assertEqual(human_duration_to_sec("12m"), 12 * 60)
        self.assertEqual(human_duration_to_sec("12 m"), 12 * 60)

    def test_minutes_seconds(self):
        self.assertEqual(human_duration_to_sec("2m:5s"), 125)
        self.assertEqual(human_duration_to_sec("2m5s"), 125)

    def test_minutes_seconds_millis(self):
        self.assertEqual(human_duration_to_sec("2m:5s:120ms"), 125.12)
        self.assertEqual(human_duration_to_sec("2m5s120ms"), 125.12)


if __name__ == '__main__':
    import os
    if os.environ.get('UNITTEST', '') == '1':
        unittest.main()
    else:
        import io

        # Ignore any Unicode errors
        unicode_errors = 'replace'
        stdin = io.TextIOWrapper(sys.stdin.buffer, errors=unicode_errors)

        parser = argparse.ArgumentParser()
        parser.add_argument(
            'file',
            type=argparse.FileType(
                'r',
                errors=unicode_errors),
            nargs='?',
            default=stdin)
        main(**vars(parser.parse_args()))
