#!/usr/bin/env python3

#   Copyright 2018-Present Couchbase, Inc.
#
#   Use of this software is governed by the Business Source License included
#   in the file licenses/BSL-Couchbase.txt.  As of the Change Date specified
#   in that file, in accordance with the Business Source License, use of this
#   software will be governed by the Apache License, Version 2.0, included in
#   the file licenses/APL2.txt.
#

"""Parses a memcached log file for 'Slow op' and 'Slow runtime' warnings, and
+converts into a Google Trace Event Format file
(https://docs.google.com/document/d/1CvAClvFfyA5R-PhYUmn5OOQtYMH4h6I0nSsKchNAySU/preview#heading=h.yr4qxyxotyw).

Usage:
    cat memcached.log | kv_slow_ops_2_gtrace > trace.json
    <open Chrome -> chrome://tracing -> Load 'trace.json'
"""

from datetime import datetime
import fileinput
import json
import re
import unittest
import sys


first = True
system_clock_steady_clock_delta = None

# Dictionary tracking which threads have already been 'seen' at least once,
# so we only emit their metadata once.
thread_seen = dict()


def iso8601_to_epoch(ts):
    """Convert an ISO 8601 time string to seconds since unix epoch."""
    return datetime.strptime(ts, '%Y-%m-%dT%H:%M:%S.%f%z').timestamp()


def human_duration_to_sec(h):
    """Converts a human-readable duration to floating-point seconds.
    e.g.
        2 s -> 2
        123 ms -> 0.123
    """
    (value, unit) = h.split(' ')
    if unit == "s":
        return float(value)
    if unit == "ms":
        return float(value) / 1000
    raise ValueError("Unhandled unit '{}'".format(unit))


def update_steady_clock_offset(system_clock, steady_clock):
    """To be able to plot Slow task runtimes (see parse_slow_task_runtime) on
    the same timeline as Slow operations, we need to map the system-clock
    timestamp to the steady-clock timestamp used by Slow operations.
    In theory system clock can drift relative to steady-clock, so re-calculate
    the delta for each "Slow operation" encountered to minimise any
    inaccuracies.
    """
    global system_clock_steady_clock_delta
    system_epoch = iso8601_to_epoch(system_clock)
    system_clock_steady_clock_delta = steady_clock - system_epoch


def print_event(event):
    """Converts the event to a JSON object and prints to stdout, adding commas
    between each object to make a JSON list.
    """
    global first
    if not first:
        print(',', end=' ')
    first = False
    print(json.dumps(event))


def parse_slow_operation(node, m):
    """Parses a log line for a 'Slow operation', outputting to stdout a Google
     Chrome tracing event for each span parsed.
    """
    time = m.group(1)
    fd = m.group(2)
    slow_op = parse_slow_op(m.group(3))

    # Set the common fields for all events for this operation
    common = dict()
    common['cat'] = slow_op['command']
    common['ph'] = 'X'
    # clientID "should" be a tuple of "connectionID/opaque", but
    # not all clients obey this convention.
    # As such, we extract the opaque here "just in case" - as older
    # server releases didn't report the full request packet, but if we
    # do have packet.opaque then use that in preference.
    cid = slow_op['cid']
    (connection_id, opaque) = cid.rsplit('/', 1)
    opaque = int(opaque, 16)
    if 'opaque' in slow_op['packet']:
        opaque = slow_op['packet']['opaque']
    common['pid'] = connection_id

    # Build a trace event from each span in the slow op.
    for span in slow_op['trace'].split():
        (name, value) = span.split('=')
        (start_ns, dur) = map(int, value.split(':'))
        # MB-43617: Ignore any spans which have zero time and duration;
        # they are spurious due to missing copy-elision when recording
        # Spans.
        if not (start_ns or dur):
            continue
        event = dict()
        event.update(common)
        event['name'] = name
        # Timestamp and duration need to be in microseconds.
        event['ts'] = start_ns / 1000
        event['dur'] = dur
        # For the top-level 'request' event, include additional request
        # details (redundant to repeat for every event).
        if name == 'request':
            event['args'] = {'opaque': opaque,
                             'fd': fd,
                             'peer': slow_op['peer']}
            event['args']['key'] = slow_op['packet']['key']
            event['args']['bucket'] = slow_op['bucket']
            event['args']['vbucket'] = slow_op['packet']['vbucket']
            event['args']['time'] = time
            # worker_tid added in v7.1.0.
            if 'worker_tid' in slow_op:
                event['args']['worker_tid'] = slow_op['worker_tid']

            # Calculate end of event in seconds according to steady_clock,
            # then update clock offset.
            ts_end_sec = (event['ts'] + event['dur']) / 1000000
            update_steady_clock_offset(time, ts_end_sec)
        if node:
            if 'args' not in event:
                event['args'] = dict()
            event['args']['node'] = node
        print_event(event)


def parse_slow_task_runtime(m):
    (time, bucket, task, thread, runtime) = m.groups()

    event = dict()
    event['cat'] = "Slow runtime"
    event['name'] = task
    event['ph'] = "X"
    event['pid'] = thread

    # Convert log message timestamp to same time domain as Slow operations
    # (steady_clock seconds)
    timestamp_epoch = iso8601_to_epoch(time)
    timestamp_steady_clock = timestamp_epoch + system_clock_steady_clock_delta
    runtime_sec = human_duration_to_sec(runtime)

    # Timestamps and duration are expressed in microseconds
    event['ts'] = (timestamp_steady_clock - runtime_sec) * 1000000
    event['dur'] = runtime_sec * 1000000
    event['args'] = {'time': time}
    print_event(event)

    if thread not in thread_seen:
        meta = dict()
        meta['name'] = "process_sort_index"
        meta['ph'] = "M"
        meta['pid'] = thread
        meta['args'] = {'sort_index': -1}
        print_event(meta)
        thread_seen[thread] = True


def parse_slow_op(trace):
    """
    Parse the trace JSON from the slow operation log.
    """

    # In some cases, we might have improperly encoded values in the output.
    # These values are JSON objects wrapped in quotes, but the strings inside
    # the objects are unescaped and cause deserialisation to fail.

    # The cid object with IPv4, potentially with port number partially /
    # entirely omitted, and potentially without closing JSON object '}'.
    trace = re.sub('"{"ip":"([\d\.]+)","port":(\d*)}?\/(\w+)"',
                   '"\\1:\\2/\\3"', trace)
    # The cid object with IPv6.
    # This can be quite corrupted:
    # - potentially with only part of IPv6 address
    # - potentially with only part of / none of port
    # - potentially without closing JSON object '}'
    trace = re.sub('"{"ip":"([0-9a-fA-F\.:]+)"?,?"?p?o?r?t?"?:?(\d*)}?\/(\w+)"',
                   '"[\\1]:\\2/\\3"', trace)
    # Peer object with IPv4
    trace = re.sub('"{"ip":"([\d\.]+)","port":(\d+)}"',
                   '"\\1:\\2"', trace)
    # Peer object with IPv6
    trace = re.sub('"{"ip":"([0-9a-fA-F\.:]+)","port":(\d+)}"',
                   '"[\\1]:\\2"', trace)

    return json.loads(trace)


def main():
    print('[')
    global first
    node = ""
    skipped_slow_runtime_events = 0
    for line in fileinput.input():
        # Check if we're parsing logs from multiple nodes, extracting data for rebasing timestamps if we are.
        #
        # Match strings beginning with a cbcollect filepath, such as:
        # cbcollect_info_ns_1@ip-or-hostname_20220722-111444/memcached.log:[log line from file...]
        # and capture ip-or-hostname to uniquely identify each node.
        if node or first:
            m = re.search("^cbcollect_.*@(.+?)_.*", line)
            if m:
                node = m.group(1)
        # Match ISO 8601 formatted log timestamps. Example:
        # 2022-07-20T01:58:01.316016+03:00
        time_regex = "(\d{4}-\d{2}-\d{2}T[0-9:.+-]+)"
        m = re.search(time_regex + " WARNING (\d+): Slow operation. (.*)", line)
        if m:
            parse_slow_operation(node,m)
            continue
        # Check for "Slow runtime" messages from tasks, e.g.
        #   2022-11-21T18:06:56.188574-05:00 WARNING (No Engine) Slow runtime for 'Compact DB file 421' on thread WriterPool3: 32 s
        m = re.search(
            time_regex + " WARNING \(([^)]+)\) Slow runtime for '(.+)' on thread (.+): (.+)",
            line)
        if m:
            if system_clock_steady_clock_delta:
                parse_slow_task_runtime(m)
            else:
                skipped_slow_runtime_events += 1
    print(']')
    if skipped_slow_runtime_events > 0:
        print(("Warning: Failed to generate traces for {} 'Slow runtime' " +
               "event{} as system_clock delta is unset.\nEnsure there is " +
               "at least one 'Slow operation' parsed prior to the " +
               "Slow runtime event(s) if you want to include them.").format(
            skipped_slow_runtime_events,
            "s" if skipped_slow_runtime_events else ""),
            file=sys.stderr)


##
## ---------------------- Unit tests ------------------------------------------
##

class TestParsing(unittest.TestCase):

    def test_6_6_2(self):
        test_string = (
            r'{"cid":"7E/FF/20","duration":"11 s","trace":"request=63:11 '
            r'get=0:0 bg.wait=6308229496700500:55 get=6308229496693730:41 '
            r'bg.load=6308229496755836:11232001 get=0:0 get=630824072879295:11"'
            r',"command":"GET","peer":"{"ip":"0.0.0.0","port":57324}","bucket":'
            r'"b","packet":{"bodylen":49,"cas":0,"datatype":"raw","extlen":0,"'
            r'key":"<ud>c0ca65a1438baaab6c8a156b05d3b08d91bfacd7</ud>",'
            r'"keylen":49,"magic":"ClientRequest","opaque":513417216,'
            r'"opcode":"GET","vbucket":318}}'
        )
        obj = parse_slow_op(test_string)
        self.assertEqual(obj['cid'], '7E/FF/20')
        self.assertEqual(obj['peer'], '0.0.0.0:57324')

    def test_6_6_5(self):
        test_string = (
            r'{"cid":"{"ip":"0.0.0.0","port":42678}/ef","duration":"521 ms",'
            r'"trace":"request=6070831171407831:521890","command":"STAT",'
            r'"peer":"{"ip":"0.0.0.0","port":42678}","bucket":"a","packet":'
            r'{"bodylen":3,"cas":0,"datatype":"raw","extlen":0,"key":'
            r'"<ud>dcp</ud>","keylen":3,"magic":"ClientRequest",'
            r'"opaque":3724742144,"opcode":"STAT","vbucket":0}}'
        )
        obj = parse_slow_op(test_string)
        self.assertEqual(obj['cid'], '0.0.0.0:42678/ef')
        self.assertEqual(obj['peer'], '0.0.0.0:42678')

    def test_6_6_5_capped_connId(self):
        """In 6.6.5, if the client doesn't specify a connectionId via HELO, KV
        generates a JSON from IP and port. However, KV only stores the first
        34 bytes of the connectionId - so the JSON object can get cropped.
        Worst case is an IP where each octet is >=100 and a 5 digit port
        is 40 characters (including nul-termination):

            {"ip": "111.222.333.444", "port":55555}
                                             XXXXXX

        We can therefore lose up to 6 bytes, losing a variable amount from
        after "port:"  (marked XXX above).
        """
        test_string = (
            r'{"cid":"{"ip":"10.149.225.86","port":4541/0","duration":"82 s",'
            r'"trace":"request=363957733255087:82601665","command":'
            r'"SASL_AUTH","peer":"{"ip":"10.149.225.86","port":45418}",'
            r'"bucket":"","packet":{"bodylen":44,"cas":0,"datatype":"raw",'
            r'"extlen":0,"key":"<ud>PLAIN</ud>","keylen":5,"magic":'
            r'"ClientRequest","opaque":0,"opcode":"SASL_AUTH","vbucket":0}}')
        obj = parse_slow_op(test_string)
        self.assertEqual(obj['cid'], '10.149.225.86:4541/0')

    def test_6_6_5_capped_connId_IPv6(self):
        """Similar to test_6_6_5_capped_connId, but with IPv6 addresses, which
        can have even worse cropping. Worst case is a full 128bit address with
        no leading zeros omitted, and 5 digit port which is 62 bytes (including
        nul-termination):

            {"ip":"1111:2222:3333:4444:5555:6666:7777:8888","port":55555}
                                             XXXXXXXXXXXXXXXXXXXXXXXXXXXX

        We can therefore lose up to 28 bytes - part of the IPv6 address and
        potentially all of the port (marked XXX above).
        """
        test_string = (
            r'{"cid":"{"ip":"2001:f547:23:4705::e156","/0","duration":'
            r'"12m:0s","trace":"request=398965059065164:720026408","command":'
            r'"SASL_AUTH","peer":"{"ip":"2001:f547:23:4705::e156","port":'
            r'55610}","bucket":"","packet":{"bodylen":46,"cas":0,"datatype":'
            r'"raw","extlen":0,"key":"<ud>PLAIN</ud>","keylen":5,"magic":'
            r'"ClientRequest","opaque":0,"opcode":"SASL_AUTH","vbucket":0}}')
        obj = parse_slow_op(test_string)
        self.assertEqual(obj['cid'], '[2001:f547:23:4705::e156]:/0')

    def test_6_6_5_IPv6(self):
        test_string = (
            r'{"cid":"{"ip":"684D:1111:222:3333:4444:5555:6:77","port":'
            r'42678}/ef","duration":"521 ms","trace":'
            r'"request=6070831171407831:521890","command":"STAT","peer":"{"ip":'
            r'"684D:1111:222:3333:4444:5555:6:77","port":42678}","bucket":"a",'
            r'"packet":{"bodylen":3,"cas":0,"datatype":"raw","extlen":0,"key":'
            r'"<ud>dcp</ud>","keylen":3,"magic":"ClientRequest",'
            r'"opaque":3724742144,"opcode":"STAT","vbucket":0}}'
        )
        obj = parse_slow_op(test_string)
        self.assertEqual(obj['cid'],
                         '[684D:1111:222:3333:4444:5555:6:77]:42678/ef')
        self.assertEqual(obj['peer'],
                         '[684D:1111:222:3333:4444:5555:6:77]:42678')

    def test_7_2_0(self):
        test_string = (
            r'{"bucket":"c","cid":"AF12/4FC/6c","command":"GET_CLUSTER_CONFIG",'
            r'"connection":"[ {\"ip\":\"0.0.0.0\",\"port\":54300} - '
            r'{\"ip\":\"0.0.0.0\",\"port\":11207} (<ud>dfs</ud>) ]","duration":'
            r'"580 ms","packet":{"bodylen":0,"cas":0,"datatype":"raw",'
            r'"extlen":0,"key":"<ud></ud>","keylen":0,"magic":"ClientRequest",'
            r'"opaque":693831680,"opcode":"GET_CLUSTER_CONFIG","vbucket":0},'
            r'"peer":{"ip":"0.0.0.0","port":54300},"response":"Success",'
            r'"trace":"request=10811483103351:580237 execute=108120633258:15",'
            r'"worker_tid":281472829188416}'
        )
        obj = parse_slow_op(test_string)
        self.assertEqual(obj['cid'], 'AF12/4FC/6c')
        self.assertEqual(obj['peer']['ip'], '0.0.0.0')
        self.assertEqual(obj['peer']['port'], 54300)


if __name__ == '__main__':
    import os
    if os.environ.get('UNITTEST', '') == '1':
        unittest.main()
    else:
        main()
